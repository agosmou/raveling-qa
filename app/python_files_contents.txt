### File: ./app.py ###
# app.py
import math
import streamlit as st
import pandas as pd
from streamlit_extras.stylable_container import stylable_container
from io import BytesIO
from PIL import Image

from models.model_repository import ModelRepository
from services.feature_extractor import RavelingFeatureExtractor
from services.prediction_service import PredictionService
from utils.image_utils import resize_image
from utils.database import (
    update_image_prediction,
    initialize_db, 
    clear_database,
    insert_csv,
    fetch_csv,
    update_ground_truth,
    insert_image,
    delete_image,
    fetch_images
)

def reset_app():
    """Clears the database and resets the session state."""
    clear_database()
    # Reset all relevant session state variables
    for key in list(st.session_state.keys()):
        if key not in ['upload_key']:  # Preserve 'upload_key' to allow resetting the uploader
            del st.session_state[key]
    st.session_state['images'] = {}
    # Increment the 'upload_key' to reset the file uploader
    st.session_state.upload_key += 1
    st.sidebar.success("All data has been cleared!")

def main():
    if 'images' not in st.session_state:
        st.session_state['images'] = {}

    # Initialize the unique key for the file uploader if not present
    if 'upload_key' not in st.session_state:
        st.session_state.upload_key = 0

    initialize_db()
    st.title("Raveling Severity Prediction App")

    # Sidebar Reset Button with Red Styling
    with st.sidebar:
        with stylable_container(
            "red",
            css_styles="""
            button {
                background-color: #FF0000 !important;
                color: white !important;
            }
            """
        ):
            if st.button("Reset App"):
                reset_app()

    # Initialize Model Repository and Prediction Service
    model_repo = ModelRepository()
    prediction_service = PredictionService(model=model_repo.model, scaler=model_repo.scaler)

    # Initialize Feature Extractor
    feature_extractor = RavelingFeatureExtractor()

    # --- Upload Ground Truth CSV First ---
    st.header("Upload Ground Truth CSV (Optional)")
    uploaded_csv = st.file_uploader(
        "Choose a CSV file. Please make sure to include the headers in your CSV file exactly as image_title and severity.",
        type=['csv'],
        key=f'csv_uploader_{st.session_state.upload_key}'
    )

    if uploaded_csv:
        try:
            csv_bytes = uploaded_csv.read()
            ground_truth_df = pd.read_csv(BytesIO(csv_bytes))
            st.session_state['ground_truth_df'] = ground_truth_df
            st.subheader("Ground Truth Data")
            st.dataframe(ground_truth_df)

            # Store CSV in database
            insert_csv(uploaded_csv.name, csv_bytes)

            # If images are already uploaded, update ground_truth_values accordingly
            if st.session_state['images']:
                for title in st.session_state['images']:
                    severity_row = ground_truth_df[ground_truth_df['image_title'] == title]
                    if not severity_row.empty:
                        severity = int(severity_row['severity'].iloc[0])
                        st.session_state['images'][title]['ground_truth'] = severity
                st.success("Ground truth values updated based on the uploaded CSV.")
        except Exception as e:
            st.error(f"Error loading CSV: {e}")
    else:
        # Fetch CSV data from database
        csv_records = fetch_csv()
        if csv_records:
            csv_id, csv_name, csv_data = csv_records[-1]  # Handle only the most recent CSV
            try:
                ground_truth_df = pd.read_csv(BytesIO(csv_data))
                st.session_state['ground_truth_df'] = ground_truth_df
                st.subheader("Loaded Ground Truth Data")
                st.dataframe(ground_truth_df)

                # If images are already uploaded, initialize ground_truth_values
                if st.session_state['images']:
                    for title in st.session_state['images']:
                        severity_row = ground_truth_df[ground_truth_df['image_title'] == title]
                        if not severity_row.empty:
                            severity = int(severity_row['severity'].iloc[0])
                            st.session_state['images'][title]['ground_truth'] = severity
            except Exception as e:
                st.error(f"Error loading CSV from database: {e}")
        else:
            st.info("No CSV data found. Please upload a CSV file.")

    # --- Upload Images ---
    st.header("Upload Images for Prediction")
    uploaded_images = st.file_uploader(
        "Choose Image Files",
        type=['png', 'jpg', 'jpeg'],
        accept_multiple_files=True,
        key=f'images_uploader_{st.session_state.upload_key}'
    )

    # Load images from the database if available and no new upload
    if not uploaded_images and not st.session_state['images']:
        db_images = fetch_images()
        if db_images:
            for img in db_images:
                _, image_title, image_data, prediction, ground_truth = img
                try:
                    image = Image.open(BytesIO(image_data)).convert('RGB')  # Now, Image is defined
                    resized_image = resize_image(image)
                    st.session_state['images'][image_title] = {
                        'image': resized_image,
                        'image_data': image_data,  # Store original image bytes
                        'features': None,          # Initialize as None; will be extracted when running predictions
                        'prediction': prediction if prediction is not None else "None",
                        'ground_truth': ground_truth
                    }
                except Exception as e:
                    st.error(f"Error loading image {image_title} from database: {e}")

    # Process images only once and store in session_state
    if uploaded_images:
        for uploaded_file in uploaded_images:
            try:
                # Read image data as bytes
                image_data = uploaded_file.getvalue()  # bytes

                # Validate the image before processing
                try:
                    Image.open(BytesIO(image_data))
                except Exception:
                    st.error(f"The file {uploaded_file.name} is not a valid image.")
                    continue  # Skip to the next file

                # Load and resize image for display
                image = Image.open(BytesIO(image_data)).convert('RGB')  # Convert to RGB for display
                resized_image = resize_image(image)
                
                # Initialize ground truth
                if 'ground_truth_df' in st.session_state:
                    ground_truth_df = st.session_state['ground_truth_df']
                    severity_row = ground_truth_df[ground_truth_df['image_title'] == uploaded_file.name]
                    ground_truth = int(severity_row['severity'].iloc[0]) if not severity_row.empty else 0
                else:
                    ground_truth = 0

                # Store in session state without features but with image_data
                st.session_state['images'][uploaded_file.name] = {
                    'image': resized_image,
                    'image_data': image_data,  # Store original image bytes
                    'features': None,          # Placeholder for features to be extracted later
                    'prediction': "None",
                    'ground_truth': ground_truth
                }

                # Insert images into the database with prediction as None
                try:
                    insert_image(
                        image_title=uploaded_file.name,
                        image_data=image_data,  # Use the already read image_data
                        prediction=None,        # Set prediction to None initially
                        ground_truth=ground_truth
                    )
                except Exception as e:
                    st.error(f"Error inserting image {uploaded_file.name} into database: {e}")

            except Exception as e:
                st.error(f"Error processing {uploaded_file.name}: {e}")

    # Add a "Run Predictions" button with Green Styling
    with stylable_container(
        "green",
        css_styles="""
        button {
            background-color: #00FF00 !important;
            color: black !important;
        }
        """
    ):
        if st.button("Run Predictions"):
            if st.session_state['images']:
                try:
                    # Initialize list to hold feature vectors
                    features_list = []
                    image_titles = []
                    
                    # Initialize progress spinner
                    with st.spinner("Extracting image features and running raveling severity predictions. Please wait as this may take a few minutes..."):
                        for title, data in st.session_state['images'].items():
                            # Check if features are already extracted
                            if data['features'] is None:
                                # Extract features using original image bytes
                                feature_vector = feature_extractor.extract_features(data['image_data'])
                                st.session_state['images'][title]['features'] = feature_vector
                            else:
                                feature_vector = data['features']
                            
                            features_list.append(feature_vector)
                            image_titles.append(title)
                    
                    # Run predictions
                    predictions = prediction_service.predict(features_list)
                    
                    # Update predictions in session state and database
                    for idx, title in enumerate(image_titles):
                        st.session_state['images'][title]['prediction'] = predictions[idx]
                        update_image_prediction(title, predictions[idx])
                    
                    st.success("Predictions have been successfully updated.")
                except Exception as e:
                    st.error(f"Error running predictions: {e}")
            else:
                st.warning("No images available for prediction.")

    # Define pagination for displaying images
    if st.session_state['images']:
        image_titles = sorted(st.session_state['images'].keys())
        total_images = len(image_titles)
        num_cols = 4  # Number of images per row
        images_per_page = num_cols * 3  # 3 rows per page
        total_pages = math.ceil(total_images / images_per_page)

        # Initialize page number if not in session state
        if 'current_page' not in st.session_state:
            st.session_state['current_page'] = 0

        # Determine the images to display for the current page
        start_idx = st.session_state['current_page'] * images_per_page
        end_idx = start_idx + images_per_page
        page_image_titles = image_titles[start_idx:end_idx]

        # Create layout columns for the current page
        cols = st.columns(num_cols)

        # Display images with predictions, ground truth inputs, and delete buttons
        for idx, image_title in enumerate(page_image_titles):
            col_idx = idx % num_cols
            with cols[col_idx]:
                image_data = st.session_state['images'][image_title]['image']
                prediction = st.session_state['images'][image_title]['prediction']
                ground_truth = st.session_state['images'][image_title]['ground_truth']

                # Display the image
                st.image(image_data, caption=image_title, use_column_width=True)
                st.write(f"**Predicted Severity:** {prediction}")

                # Ground Truth Input
                ground_truth_input = st.number_input(
                    f"Ground Truth for {image_title}",
                    min_value=0,
                    max_value=3,
                    value=ground_truth,
                    key=f"ground_truth_{image_title}_",
                )

                # Update ground truth in session state and database
                st.session_state['images'][image_title]['ground_truth'] = ground_truth_input
                try:
                    update_ground_truth(
                        ground_truth=ground_truth_input,
                        image_title=image_title
                    )
                except Exception as e:
                    st.error(f"Error updating ground truth for {image_title}: {e}")

                # Delete Button
                if st.button("Delete Image", key=f"delete_{image_title}"):
                    try:
                        # Delete from database
                        delete_image(image_title)
                        # Delete from session state
                        del st.session_state['images'][image_title]
                        st.success(f"Image '{image_title}' has been deleted successfully.")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error deleting image '{image_title}': {e}")
                
                st.markdown("---")

        # Display page navigation buttons
        st.write(f"Showing page {st.session_state['current_page'] + 1} of {total_pages}")

        # Allow user to jump to a specific page
        st.markdown("### Jump to Page")
        page_input = st.number_input(
            "Enter page number:",
            min_value=1,
            max_value=total_pages,
            value=st.session_state['current_page'] + 1,
            step=1,
            key="page_input",
            help="Type the page number you want to navigate to."
        )

        # Update the current page based on user input
        if st.session_state['current_page'] != page_input - 1:
            st.session_state['current_page'] = page_input - 1

        # Previous and Next Buttons
        col1, col2, col3 = st.columns([1, 2, 1])
        with col1:
            prev_clicked = st.button("◀ Previous")
            if prev_clicked and st.session_state['current_page'] > 0:
                st.session_state['current_page'] -= 1 
        with col3:
            next_clicked = st.button("Next ▶")
            if next_clicked and st.session_state['current_page'] < total_pages - 1:
                st.session_state['current_page'] += 1 

        st.markdown("---")

        # Create and download predictions CSV
        if st.button("Generate Ground Truth vs Predictions CSV"):
            try:
                # Create results dataframe using current values from session state
                results_df = pd.DataFrame([
                    {
                        'image_title': title,
                        'predicted_severity': st.session_state['images'][title]['prediction'],
                        'ground_truth_severity': st.session_state['images'][title]['ground_truth']
                    }
                    for title in image_titles
                ])
                
                # Convert 'None' to None type for proper handling
                results_df['predicted_severity'] = results_df['predicted_severity'].replace('None', None)
                
                # Convert to numeric, handling any non-numeric values
                results_df['predicted_severity'] = pd.to_numeric(results_df['predicted_severity'], errors='coerce')
                results_df['ground_truth_severity'] = pd.to_numeric(results_df['ground_truth_severity'], errors='coerce')
                
                csv = results_df.to_csv(index=False)
                
                st.download_button(
                    label="Download Ground Truth vs Predictions CSV",
                    data=csv,
                    file_name="gt_vs_pred.csv",
                    mime="text/csv"
                )
                st.success("CSV generated successfully.")
            except Exception as e:
                st.error(f"Error generating CSV: {e}")

        # Show comparison if ground truth exists
        if 'ground_truth_df' in st.session_state:
            try:
                # Create comparison dataframe using current values
                current_predictions = pd.DataFrame([
                    {
                        'image_title': title,
                        'Predicted Severity': st.session_state['images'][title]['prediction'],
                        'Ground Truth Severity': st.session_state['images'][title]['ground_truth']
                    }
                    for title in image_titles
                ])
                
                # Convert 'None' strings to None type
                current_predictions['Predicted Severity'] = current_predictions['Predicted Severity'].replace('None', None)
                
                # Convert to numeric, handling any non-numeric values
                current_predictions['Predicted Severity'] = pd.to_numeric(current_predictions['Predicted Severity'], errors='coerce')
                current_predictions['Ground Truth Severity'] = pd.to_numeric(current_predictions['Ground Truth Severity'], errors='coerce')
                
                st.subheader("Comparison of Ground Truth and Predictions")
                
                # Create a styled dataframe for display
                styled_df = current_predictions.style.format({
                    'Predicted Severity': '{:.0f}',
                    'Ground Truth Severity': '{:.0f}'
                })
                
                st.dataframe(styled_df)
                
            except Exception as e:
                st.error(f"Error creating comparison table: {e}")

    # Add legend in the sidebar
    st.sidebar.markdown("""
    ### Severity Legend
    - **0** = None
    - **1** = Low
    - **2** = Medium
    - **3** = Severe
    """)

if __name__ == "__main__":
    main()


### File: ./models/model_repository.py ###
# models/model_repository.py
import os
import pickle
import streamlit as st

class ModelRepository:
    def __init__(self, model_path: str = None, scaler_path: str = None):
        # Determine the directory where this script resides
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # If no paths are provided, default to the 'models' directory
        if model_path is None:
            model_path = os.path.join(current_dir, 'rf_model.pkl')
        if scaler_path is None:
            scaler_path = os.path.join(current_dir, 'scaler.pkl')
        
        self.model_path = model_path
        self.scaler_path = scaler_path
        self.model, self.scaler = self.load_model_and_scaler()

    @st.cache_resource
    def load_model_and_scaler(_self):
        # Check if the model file exists
        if not os.path.exists(_self.model_path):
            raise FileNotFoundError(f"Model file not found at {_self.model_path}")
        if not os.path.exists(_self.scaler_path):
            raise FileNotFoundError(f"Scaler file not found at {_self.scaler_path}")
        
        with open(_self.model_path, 'rb') as model_file:
            model = pickle.load(model_file)
        with open(_self.scaler_path, 'rb') as scaler_file:
            scaler = pickle.load(scaler_file)
        return model, scaler


### File: ./utils/image_utils.py ###
# utils/image_utils.py
from PIL import Image

def resize_image(image: Image.Image, max_size: int = 300) -> Image.Image:
    """Resize image maintaining aspect ratio."""
    ratio = max_size / max(image.size)
    new_size = tuple([int(x * ratio) for x in image.size])
    return image.resize(new_size, Image.Resampling.LANCZOS)


### File: ./utils/database.py ###
# utils/database.py
from typing import List, Tuple
import pandas as pd
import sqlite3

DATABASE_NAME = "app_data.db"

def get_db_connection():
    """Establishes and returns a connection to the SQLite database."""
    conn = sqlite3.connect(DATABASE_NAME)
    return conn

def initialize_db():
    """Initializes the database by creating necessary tables with appropriate constraints."""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Create images table with UNIQUE constraint on image_title
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS images (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            image_title TEXT NOT NULL UNIQUE,
            image_data BLOB NOT NULL,
            prediction INTEGER,
            ground_truth INTEGER DEFAULT 0
        )
    ''')
    
    # Create CSV table with UNIQUE constraint on csv_name
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS csv_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            csv_name TEXT NOT NULL UNIQUE,
            csv_data BLOB NOT NULL
        )
    ''')
    
    conn.commit()
    conn.close()

def clear_database():
    """Clears all data from the images and csv_data tables."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('DELETE FROM images')
    cursor.execute('DELETE FROM csv_data')
    conn.commit()
    conn.close()

def fetch_images() -> List[Tuple]:
    """Fetches all images from the database along with their predictions and ground_truth values."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT id, image_title, image_data, prediction, ground_truth FROM images')
    images = cursor.fetchall()
    conn.close()
    return images

def insert_image(image_title: str, image_data: bytes, prediction: int = None, ground_truth: int = 0):
    """
    Inserts a new image into the database.
    If the image already exists, updates its prediction and ground_truth.
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        cursor.execute('''
            INSERT INTO images (image_title, image_data, prediction, ground_truth)
            VALUES (?, ?, ?, ?)
        ''', (image_title, image_data, prediction, ground_truth))
        conn.commit()
    except sqlite3.IntegrityError:
        # Image already exists, update prediction and ground_truth
        cursor.execute('''
            UPDATE images
            SET image_data = ?, prediction = ?, ground_truth = ?
            WHERE image_title = ?
        ''', (image_data, prediction, ground_truth, image_title))
        conn.commit()
    finally:
        conn.close()

def delete_image(image_title: str):
    """
    deletes image from app/db
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        "DELETE FROM images WHERE image_title = ?",
        (image_title,)
    )
    conn.commit()
    conn.close()

def insert_csv(csv_name: str, csv_data: bytes):
    """
    Inserts a new CSV into the database.
    If the CSV already exists, updates its data.
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        cursor.execute('''
            INSERT INTO csv_data (csv_name, csv_data)
            VALUES (?, ?)
        ''', (csv_name, csv_data))
        conn.commit()
    except sqlite3.IntegrityError:
        # CSV already exists, update csv_data
        cursor.execute('''
            UPDATE csv_data
            SET csv_data = ?
            WHERE csv_name = ?
        ''', (csv_data, csv_name))
        conn.commit()
    finally:
        conn.close()

def fetch_csv() -> List[Tuple]:
    """Fetches all CSVs from the database."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT id, csv_name, csv_data FROM csv_data')
    csvs = cursor.fetchall()  # Corrected: Added parentheses
    conn.close()
    return csvs

def update_ground_truth(ground_truth: int, image_title: str):
    """Updates ground truth for a specific image."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE images SET ground_truth = ? WHERE image_title = ?",
        (ground_truth, image_title)
    )
    conn.commit()
    conn.close()

def update_image_prediction(image_title, prediction):
    """Helper function to update prediction in the database."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE images SET prediction = ? WHERE image_title = ?",
        (prediction, image_title)
    )
    conn.commit()
    conn.close()

### File: ./services/feature_extractor.py ###
# services/feature_extractor.py
import math
import time
import streamlit as st
import numpy as np
from scipy.stats import iqr, skew, kurtosis, norm
from PIL import Image
from io import BytesIO

class RavelingFeatureExtractor:
    def __init__(self, p_size=25):
        self.p_size = p_size
        self.height = 417
        self.width = 520
        self.h_patch = math.ceil(self.height / self.p_size)
        self.w_patch = math.ceil(self.width / self.p_size)
        
        # Precompute linspace arrays for distribution features
        self.std_linspace = np.linspace(0, 30, 100)
        self.iqr_linspace = np.linspace(0, 30, 100)
        self.mean_linspace = np.linspace(100, 140, 100)
        self.rms_linspace = np.linspace(0, 20, 100)
        self.skew_linspace = np.linspace(-8, 5, 100)
        self.kurt_linspace = np.linspace(-5, 10, 100)
        
    def load_and_prepare_image(self, uploaded_file: bytes) -> Image.Image:
        """
        Load image from uploaded file bytes and convert to grayscale.
        
        Args:
            uploaded_file (bytes): Image file in bytes.
        
        Returns:
            Image.Image: Grayscale PIL Image.
        
        Raises:
            ValueError: If the image cannot be opened or converted.
        """
        try:
            image = Image.open(BytesIO(uploaded_file)).convert('L')  # Convert to grayscale
            return image
        except Exception as e:
            raise ValueError(f"Failed to load image: {e}")
        
    def resize_image(self, img: Image.Image, target_size=(1250, 1040)) -> Image.Image:
        """
        Resize image to target size using PIL without maintaining aspect ratio.
        
        Args:
            img (Image.Image): PIL Image to resize.
            target_size (tuple): Desired size as (width, height).
        
        Returns:
            Image.Image: Resized PIL Image.
        """
        try:
            resized_img = img.resize(target_size, Image.Resampling.LANCZOS)
            return resized_img
        except Exception as e:
            raise ValueError(f"Failed to resize image: {e}")
        
    def split_into_smaller_images(self, img: Image.Image) -> list:
        """
        Split the larger image into 6 smaller images as per mentor's featurization.
        Assumes the image has been resized to (1250, 1040).
        
        Args:
            img (Image.Image): Resized PIL Image.
        
        Returns:
            list: List of 6 NumPy arrays representing smaller images.
        
        Raises:
            ValueError: If the image dimensions are incorrect.
        """
        if img.size != (1250, 1040):
            raise ValueError(f"Image has incorrect size: {img.size}. Expected (1250, 1040).")
        
        img_np = np.array(img)
        smaller_images = [
            img_np[0:417, 0:520],
            img_np[0:417, 520:1040],
            img_np[417:834, 0:520],
            img_np[417:834, 520:1040],
            img_np[834:1250, 0:520],
            img_np[834:1250, 520:1040]
        ]
        return smaller_images
        
    def calculate_global_features(self, img: np.ndarray) -> np.ndarray:
        """
        Calculate the 6 global statistical features for a given image patch.
        
        Args:
            img (np.ndarray): Grayscale image patch.
        
        Returns:
            np.ndarray: Array of 6 global features.
        """
        std_val = np.std(img)
        iqr_val = iqr(img)
        mean_val = np.mean(np.abs(img))
        rms_val = np.sqrt(np.mean(img ** 2))
        skew_val = skew(img.flatten())
        kurt_val = kurtosis(img.flatten())
        global_features = np.array([std_val, iqr_val, mean_val, rms_val, skew_val, kurt_val])
        return global_features
        
    def calculate_patch_features(self, img: np.ndarray) -> np.ndarray:
        """
        Calculate distribution features from patches using vectorized operations.
        
        Args:
            img (np.ndarray): Grayscale image patch.
        
        Returns:
            np.ndarray: Concatenated array of 600 distribution features.
        """
        # Initialize lists to store patch-wise features
        std_val = []
        iqr_val = []
        mean_val = []
        rms_val = []
        skw_val = []
        kurt_val = []
        
        # Efficiently iterate over patches without unnecessary computations
        for h in range(self.h_patch):
            for w in range(self.w_patch):
                start_h = h * self.p_size
                end_h = min((h + 1) * self.p_size, self.height)
                start_w = w * self.p_size
                end_w = min((w + 1) * self.p_size, self.width)
                
                patch = img[start_h:end_h, start_w:end_w]
                
                if patch.size == 0:
                    continue  # Skip empty patches
                
                # Append computed features to respective lists
                std_val.append(np.std(patch))
                iqr_val.append(iqr(patch))
                mean_val.append(np.mean(np.abs(patch)))
                rms_val.append(np.sqrt(np.mean(patch ** 2)))
                skw_val.append(skew(patch.flatten()))
                kurt_val.append(kurtosis(patch.flatten()))
        
        # Convert lists to NumPy arrays for vectorized operations
        std_val = np.array(std_val)
        iqr_val = np.array(iqr_val)
        mean_val = np.array(mean_val)
        rms_val = np.array(rms_val)
        skw_val = np.array(skw_val)
        kurt_val = np.array(kurt_val)
        
        # Initialize distribution feature arrays
        std_dist = np.zeros(100)
        iqr_dist = np.zeros(100)
        mean_dist = np.zeros(100)
        rms_dist = np.zeros(100)
        skew_dist = np.zeros(100)
        kurt_dist = np.zeros(100)
        
        # Vectorized PDF computations using list comprehensions
        if std_val.size > 0:
            std_pdf = [norm(xi).pdf(self.std_linspace) for xi in std_val]
            std_dist = np.sum(std_pdf, axis=0)
        
        if iqr_val.size > 0:
            iqr_pdf = [norm(xi).pdf(self.iqr_linspace) for xi in iqr_val]
            iqr_dist = np.sum(iqr_pdf, axis=0)
        
        if mean_val.size > 0:
            mean_pdf = [norm(xi).pdf(self.mean_linspace) for xi in mean_val]
            mean_dist = np.sum(mean_pdf, axis=0)
        
        if rms_val.size > 0:
            rms_pdf = [norm(xi).pdf(self.rms_linspace) for xi in rms_val]
            rms_dist = np.sum(rms_pdf, axis=0)
        
        if skw_val.size > 0:
            skew_pdf = [norm(xi).pdf(self.skew_linspace) for xi in skw_val]
            skew_dist = np.sum(skew_pdf, axis=0)
        
        if kurt_val.size > 0:
            kurt_pdf = [norm(xi).pdf(self.kurt_linspace) for xi in kurt_val]
            kurt_dist = np.sum(kurt_pdf, axis=0)
        
        # Concatenate all distribution features
        distribution_features = np.concatenate([
            std_dist,
            iqr_dist,
            mean_dist,
            rms_dist,
            skew_dist,
            kurt_dist
        ])
        
        # Ensure the concatenated distribution_features has size 600
        if distribution_features.shape[0] != 600:
            raise ValueError(f"Distribution features have incorrect shape: {distribution_features.shape}. Expected (600,)")
        
        return distribution_features
        
    def extract_features(self, uploaded_file: bytes) -> np.ndarray:
        """
        Extract 606 features from an uploaded image:
        - 6 global statistical features per smaller image
        - 600 distribution features (100 points x 6 features) per smaller image
        Aggregated across all 6 smaller images by taking the mean.
        
        Args:
            uploaded_file (bytes): Image file in bytes.
        
        Returns:
            np.ndarray: Aggregated feature vector of length 606.
        
        Raises:
            ValueError: If any step in the feature extraction fails.
        """
        start_time = time.time()
    
        # Load and prepare image
        img = self.load_and_prepare_image(uploaded_file)
        
        # Resize image to expected size
        img = self.resize_image(img)
        
        # Split into 6 smaller images
        smaller_images = self.split_into_smaller_images(img)
        
        all_features = []
        
        for j, smaller_img in enumerate(smaller_images):
            # Calculate global features
            global_features = self.calculate_global_features(smaller_img)
            
            # Calculate distribution features
            distribution_features = self.calculate_patch_features(smaller_img)
            
            # Combine global and distribution features
            features = np.concatenate([global_features, distribution_features])
            
            # Verify combined feature vector length
            if features.shape[0] != 606:
                raise ValueError(f"Combined features for smaller image {j} have incorrect shape: {features.shape}. Expected (606,)")
            
            all_features.append(features)
        
        # Convert to NumPy array and aggregate by taking the mean across the 6 smaller images
        try:
            all_features = np.array(all_features)
            if all_features.shape != (6, 606):
                raise ValueError(f"All features array has incorrect shape: {all_features.shape}. Expected (6, 606).")
            aggregated_features = np.mean(all_features, axis=0)
            end_time = time.time()
            # Optional: Log the feature extraction time
            # st.write(f"Feature extraction time: {end_time - start_time:.2f} seconds")
        except ValueError as ve:
            raise ValueError(f"Error aggregating features: {ve}")
        
        return aggregated_features


### File: ./services/prediction_service.py ###
# services/prediction_service.py
import pandas as pd

class PredictionService:
    def __init__(self, model, scaler):
        self.model = model
        self.scaler = scaler

    def predict(self, features: list) -> list:
        X_new = pd.DataFrame(features)
        X_new_scaled = self.scaler.transform(X_new)
        predictions = self.model.predict(X_new_scaled)
        return predictions.tolist()


